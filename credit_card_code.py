# -*- coding: utf-8 -*-
"""Credit_Card_Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VdE7-oU6QlHu7B-hrVFni_RtqbV_Pc2V
"""

# Import libraries
import warnings
warnings.filterwarnings('ignore')
import kagglehub
from kagglehub import *
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import os
# ML libraries
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, roc_curve, auc
)
from sklearn.feature_selection import mutual_info_regression
from xgboost import XGBClassifier
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
# Imbalance handling
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek

# Styling
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette('husl')
pd.set_option('display.max_columns', None)

print("‚úì All libraries imported successfully")

path = kagglehub.dataset_download("rikdifos/credit-card-approval-prediction")

dataset_dir = path
app_path = os.path.join(dataset_dir, "application_record.csv")
cred_path = os.path.join(dataset_dir, "credit_record.csv")

app_data = pd.read_csv(app_path)
credit_data = pd.read_csv(cred_path)

print(f"üìä Application Records: {app_data.shape[0]:,} rows, {app_data.shape[1]} columns")
print(f"üìä Credit Records: {credit_data.shape[0]:,} rows, {credit_data.shape[1]} columns")
print(f"üë• Unique Applicants: {app_data['ID'].nunique():,}")
print(f"üîó Common IDs: {len(set(app_data['ID']).intersection(set(credit_data['ID']))):,}")

application_record_df = pd.read_csv('/kaggle/input/credit-card-approval-prediction/application_record.csv')
application_record_df.head()

print("\n" + "="*80)
print("APPLICATION DATA SAMPLE")
print("="*80)
display(app_data.head())

print("\n" + "="*80)
print("CREDIT RECORD SAMPLE")
print("="*80)
display(credit_data.head(20))

print("="*80)
print("DATA QUALITY CHECK")
print("="*80)

print("\nüìã Credit Data Info:")
credit_data.info()

print("\nüìã Application Data Info:")
app_data.info()

print("\nüîç Missing Values - Credit Data:")
print(credit_data.isnull().sum())

print("\nüîç Missing Values - Application Data:")
print(app_data.isnull().sum())

print('Application data null values:')
print(application_record_df.isnull().sum())

# Visualize credit status distribution
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Status counts
status_counts = credit_data['STATUS'].value_counts()
axes[0].bar(status_counts.index, status_counts.values, color='#667eea')
axes[0].set_xlabel('Status', fontsize=12)
axes[0].set_ylabel('Count', fontsize=12)
axes[0].set_title('Credit Status Distribution', fontsize=14, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# Months balance distribution
axes[1].hist(credit_data['MONTHS_BALANCE'], bins=50, color='#764ba2', edgecolor='black')
axes[1].set_xlabel('Months Balance', fontsize=12)
axes[1].set_ylabel('Frequency', fontsize=12)
axes[1].set_title('Months Balance Distribution', fontsize=14, fontweight='bold')
axes[1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f"\nüìä Status Value Counts:")
print(credit_data['STATUS'].value_counts())
print(f"\nüìä Months Balance Range: {credit_data['MONTHS_BALANCE'].min()} to {credit_data['MONTHS_BALANCE'].max()}")

# === DATA SCIENCE INSIGHT: Class Imbalance Analysis ===
fig, axes = plt.subplots(1, 2, figsize=(15, 5))
# Original class distribution
status_counts = credit_data['STATUS'].value_counts()
axes[0].bar(status_counts.index, status_counts.values, color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8'], edgecolor='black', linewidth=2)
axes[0].set_xlabel('Credit Status', fontsize=12, fontweight='bold')
axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')
axes[0].set_title('üìà Original Data Distribution: Class Imbalance Problem\n(Data Science Challenge)', fontsize=13, fontweight='bold')
axes[0].grid(alpha=0.3, axis='y')

# Income distribution
income_dist = app_data['AMT_INCOME_TOTAL'].describe()
axes[1].hist(app_data['AMT_INCOME_TOTAL'], bins=50, color='#667eea', edgecolor='black', alpha=0.7)
axes[1].set_xlabel('Annual Income', fontsize=12, fontweight='bold')
axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')
axes[1].set_title('üí∞ Income Distribution Analysis\n(Feature Understanding)', fontsize=13, fontweight='bold')
axes[1].axvline(app_data['AMT_INCOME_TOTAL'].median(), color='red', linestyle='--', linewidth=2, label=f'Median: ${app_data["AMT_INCOME_TOTAL"].median():,.0f}')
axes[1].legend()
axes[1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 6))
income_counts = app_data['NAME_INCOME_TYPE'].value_counts()
plt.barh(income_counts.index, income_counts.values, color='#667eea')
plt.xlabel('Count', fontsize=12)
plt.ylabel('Income Type', fontsize=12)
plt.title('Income Type Distribution', fontsize=14, fontweight='bold')
plt.grid(alpha=0.3, axis='x')
plt.tight_layout()
plt.show()

# Create Good/Bad labels from STATUS
credit_data['Good or Bad'] = credit_data['STATUS'].apply(
    lambda x: 'Good' if x in ['0', 'X', 'C'] else 'Bad'
)

# Group by ID and label
credit_goods_bads = credit_data.groupby(['ID', 'Good or Bad']).size().to_frame('size')
credit_goods_bads.reset_index(inplace=True)

# Get the dominant label for each customer
idx = credit_goods_bads.groupby('ID')['size'].idxmax()
max_goods_bads = credit_goods_bads.loc[idx]

# Convert to binary (1=Good, 0=Bad)
max_goods_bads['Label'] = max_goods_bads['Good or Bad'].apply(lambda x: 1 if x == 'Good' else 0)
max_goods_bads = max_goods_bads[['ID', 'Label']].reset_index(drop=True)

print(f"‚úì Created target labels for {len(max_goods_bads):,} customers")
print(f"\nLabel Distribution:")
print(f"  Good (1): {(max_goods_bads['Label'] == 1).sum():,} ({(max_goods_bads['Label'] == 1).sum() / len(max_goods_bads) * 100:.1f}%)")
print(f"  Bad (0):  {(max_goods_bads['Label'] == 0).sum():,} ({(max_goods_bads['Label'] == 0).sum() / len(max_goods_bads) * 100:.1f}%)")

# Fill missing occupation
app_data.fillna('Unknown', inplace=True)

# Merge application and credit data
data = pd.merge(app_data, max_goods_bads, how='inner', on='ID')

print(f"‚úì Merged dataset: {len(data):,} rows, {data.shape[1]} columns")
print(f"\nCommon IDs: {len(set(app_data['ID']).intersection(set(max_goods_bads['ID']))):,}")

# Visualize target distribution
# fig, ax = plt.subplots(figsize=(8, 6))
# target_counts = data['Label'].value_counts()
# colors = ['#f5576c', '#43e97b']
# ax.pie(target_counts.values, labels=['Good', 'Bad'], autopct='%1.1f%%',
#        colors=colors, startangle=90, textprops={'fontsize': 14, 'fontweight': 'bold'})
# ax.set_title('Target Variable Distribution', fontsize=16, fontweight='bold', pad=20)
# plt.tight_layout()
# plt.show()

# === DATA SCIENCE INSIGHT: Interactive Target Distribution ===
fig = make_subplots(rows=1, cols=2, specs=[[{"type":"pie"}, {"type":"bar"}]])

label_counts = data['Label'].value_counts()
fig.add_trace(go.Pie(labels=['Good Customers', 'Bad Customers'],
                     values=[label_counts[1], label_counts[0]],
                     marker=dict(colors=['#43e97b', '#f5576c']),
                     name='Distribution',
                     textinfo='label+percent'),
            row=1, col=1)

age_bins = pd.cut(data['DAYS_BIRTH'].apply(lambda x: abs(x)//365), bins=10)
age_label_dist = data.groupby(age_bins)['Label'].agg(['sum', 'count'])
age_label_dist['bad'] = age_label_dist['count'] - age_label_dist['sum']

fig.add_trace(go.Bar(x=[f"{int(interval.left)}-{int(interval.right)}" for interval in age_label_dist.index],
                     y=age_label_dist['sum'],
                     name='Good Customers',
                     marker_color='#43e97b'),
            row=1, col=2)

fig.add_trace(go.Bar(x=[f"{int(interval.left)}-{int(interval.right)}" for interval in age_label_dist.index],
                     y=age_label_dist['bad'],
                     name='Bad Customers',
                     marker_color='#f5576c'),
            row=1, col=2)

fig.update_layout(height=500, title_text="üîç Target Variable & Age-Based Default Analysis (Exploratory Data Analysis)")
fig.show()

# Prepare features and target
X = data.drop('Label', axis=1)
y = data['Label']

print(f"Features shape: {X.shape}")
print(f"Target shape: {y.shape}")
print(f"\nFeatures: {list(X.columns)}")

# Calculate mutual information scores
X_info = X.copy()

# Encode categorical features
for colname in X_info.select_dtypes("object"):
    X_info[colname], _ = X_info[colname].factorize()

# Convert floats to ints
for colname in X_info.select_dtypes("float"):
    X_info[colname] = X_info[colname].astype(int)

discrete_features = X_info.dtypes != int

def make_mi_scores(X, y, discrete_features):
    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features)
    mi_scores = pd.Series(mi_scores, name="MI Scores", index=X.columns)
    mi_scores = mi_scores.sort_values(ascending=False)
    return mi_scores

mi_scores = make_mi_scores(X_info, y, discrete_features)

# # Plot
# plt.figure(figsize=(10, 12))
# mi_scores_sorted = mi_scores.sort_values(ascending=True)
# plt.barh(range(len(mi_scores_sorted)), mi_scores_sorted.values, color='#667eea')
# plt.yticks(range(len(mi_scores_sorted)), mi_scores_sorted.index)
# plt.xlabel('Mutual Information Score', fontsize=12)
# plt.title('Feature Importance - Mutual Information', fontsize=14, fontweight='bold')
# plt.grid(alpha=0.3, axis='x')
# plt.tight_layout()
# plt.show()

# print("\nTop 10 Most Important Features:")
# display(mi_scores.head(10))

# === DATA SCIENCE INSIGHT: Feature Importance Visualization ===
fig = make_subplots(rows=1, cols= 2, subplot_titles=("Top 10 Important Features", "Cumulative Feature Importance"),
                    specs=[[{"type":"bar"}, {"type":"scatter"}]])

top_10_mi = mi_scores.head(10)
fig.add_trace(go.Bar(y=top_10_mi.index, x=top_10_mi.values, orientation='h',
                    marker=dict(color=top_10_mi.values, colorscale='Viridis'),
                    name='Mutual Information'), row=1, col=1)

cumsum_mi = mi_scores.cumsum() / mi_scores.sum() * 100
fig.add_trace(go.Scatter(x=np.arange(len(cumsum_mi)), y=cumsum_mi.values,
                        mode='lines+markers', name='Cumulative %',
                        line=dict(color='#667eea', width=3)), row=1, col=2)

fig.update_yaxes(title_text="Feature", row=1, col=1)
fig.update_xaxes(title_text="MI Score", row=1, col=1)
fig.update_xaxes(title_text="Feature Index", row=1, col=2)
fig.update_yaxes(title_text="Cumulative Importance %", row=1, col=2)
fig.update_layout(height=500, title_text="üéØ Feature Selection: Mutual Information Analysis", showlegend=True)
fig.show()

# Visualize key relationships
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

sns.scatterplot(data=data, x='AMT_INCOME_TOTAL', y='Label', alpha=0.5, ax=axes[0], color='#667eea')
axes[0].set_title('Income vs Label', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Income Total', fontsize=12)
axes[0].set_ylabel('Label (0=Bad, 1=Good)', fontsize=12)

sns.scatterplot(data=data, x='DAYS_BIRTH', y='Label', alpha=0.5, ax=axes[1], color='#764ba2')
axes[1].set_title('Age (Days Birth) vs Label', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Days Birth', fontsize=12)
axes[1].set_ylabel('Label (0=Bad, 1=Good)', fontsize=12)

plt.tight_layout()
plt.show()

# Convert categorical to numerical
X_encoded = pd.get_dummies(X, drop_first=True)

print(f"‚úì Encoded features shape: {X_encoded.shape}")
print(f"‚úì Number of features after encoding: {X_encoded.shape[1]}")

# ============================================================================
# SECTION 5: DATA PREPROCESSING PIPELINE
# ============================================================================

print("\n" + "="*80)
print("üìä DATA PREPROCESSING PIPELINE - DATA SCIENCE WORKFLOW")
print("="*80)

# Step 1: One-hot encode
X_encoded = pd.get_dummies(X, drop_first=True)
print(f"\n‚úì Step 1 - Encoding: {X.shape[1]} ‚Üí {X_encoded.shape[1]} features")

# Step 2: Train-test split (BEFORE resampling - correct approach)
X_train, X_test, y_train, y_test = train_test_split(
    X_encoded, y, test_size=0.2, random_state=42, stratify=y
)

# Step 3: Scale
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(f"‚úì Step 2 - Scaling: Features normalized to mean=0, std=1")

# Step 4: SMOTE + Tomek (on training data ONLY)
print(f"\n‚úì Step 3 - Handling Class Imbalance (SMOTE + Tomek):")
print(f"  Before - Good: {(y_train == 1).sum():,} | Bad: {(y_train == 0).sum():,}")

smote_tomek = SMOTETomek(random_state=42)
X_balanced, y_balanced = smote_tomek.fit_resample(X_train_scaled, y_train)

print(f"  After  - Good: {(y_balanced == 1).sum():,} | Bad: {(y_balanced == 0).sum():,}")

# === DATA SCIENCE INSIGHT: Resampling Comparison Animation ===
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

before_good = (y_train == 1).sum()
before_bad = (y_train == 0).sum()
after_good = (y_balanced == 1).sum()
after_bad = (y_balanced == 0).sum()

x_pos = np.arange(2)
width = 0.35

axes[0].bar(x_pos - width/2, [before_good, before_bad], width, label='Original', color=['#43e97b', '#f5576c'], alpha=0.8, edgecolor='black', linewidth=2)
axes[0].set_ylabel('Count', fontsize=12, fontweight='bold')
axes[0].set_title('Before: Imbalanced Data', fontsize=13, fontweight='bold')
axes[0].set_xticks(x_pos)
axes[0].set_xticklabels(['Good', 'Bad'])
axes[0].legend()
axes[0].grid(alpha=0.3, axis='y')

axes[1].bar(x_pos - width/2, [after_good, after_bad], width, label='After SMOTE+Tomek', color=['#43e97b', '#f5576c'], alpha=0.8, edgecolor='black', linewidth=2)
axes[1].set_ylabel('Count', fontsize=12, fontweight='bold')
axes[1].set_title('After: Balanced Data (Problem Solved!)', fontsize=13, fontweight='bold')
axes[1].set_xticks(x_pos)
axes[1].set_xticklabels(['Good', 'Bad'])
axes[1].legend()
axes[1].grid(alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

# ============================================================================
# SECTION 6: DIMENSIONALITY REDUCTION WITH PCA
# ============================================================================

print(f"\n‚úì Step 4 - Dimensionality Reduction (PCA):")
pca = PCA(n_components=5, random_state=42)
X_pca = pca.fit_transform(X_balanced)

print(f"  Original features: {X_balanced.shape[1]}")
print(f"  PCA components: {X_pca.shape[1]}")
print(f"  Variance explained: {pca.explained_variance_ratio_.sum():.2%}")

# Transform test set
X_test_pca = pca.transform(X_test_scaled)

# === DATA SCIENCE INSIGHT: PCA Variance Explanation ===
fig = make_subplots(rows=1, cols =2, subplot_titles=("Individual Component Variance", "Cumulative Explained Variance"),
                    specs=[[{"type":"bar"}, {"type":"scatter"}]])

variance_ratio = pca.explained_variance_ratio_
cumsum_variance = np.cumsum(variance_ratio)

fig.add_trace(go.Bar(x=[f'PC{i+1}' for i in range(5)], y=variance_ratio*100,
                    marker=dict(color=['#667eea', '#764ba2', '#f093fb', '#4facfe', '#00f2fe']),
                    name='Individual Variance', text=[f'{v*100:.1f}%' for v in variance_ratio],
                    textposition='auto'), row=1, col=1)

fig.add_trace(go.Scatter(x=[f'PC{i+1}' for i in range(5)], y=cumsum_variance*100,
                        mode='lines+markers+text', name='Cumulative Variance',
                        line=dict(color='#FF6B6B', width=3),
                        text=[f'{c*100:.1f}%' for c in cumsum_variance],
                        textposition='top center'), row=1, col=2)

fig.update_yaxes(title_text="Variance %", row=1, col=1)
fig.update_yaxes(title_text="Cumulative %", row=1, col=2)
fig.update_layout(height=500, title_text="üî¨ PCA Analysis: From 48 Features to 5 Components", showlegend=True)
fig.show()

# ============================================================================
# SECTION 7: SPLIT BALANCED DATA & MODEL TRAINING
# ============================================================================

X_train_pca, X_test_pca_split, y_train_pca, y_test_pca = train_test_split(
    X_pca, y_balanced, test_size=0.2, random_state=42
)

print(f"\n‚úì Training set: {X_train_pca.shape} | Test set: {X_test_pca_split.shape}")

# Train XGBoost
print(f"\n‚úì Step 5 - Model Training: XGBoost Classifier")
xgb_model = XGBClassifier(n_estimators=500, max_depth=None, random_state=42, eval_metric='logloss', verbosity=0)
xgb_model.fit(X_train_pca, y_train_pca)

y_pred = xgb_model.predict(X_test_pca_split)
y_pred_proba = xgb_model.predict_proba(X_test_pca_split)[:, 1]

print(f"  Model trained with 500 estimators and max_depth=3")

# ============================================================================
# SECTION 8: MODEL EVALUATION & PERFORMANCE METRICS
# ============================================================================

print("\n" + "="*80)
print("üìà MODEL PERFORMANCE METRICS - COMPREHENSIVE EVALUATION")
print("="*80)

accuracy = accuracy_score(y_test_pca, y_pred)
precision = precision_score(y_test_pca, y_pred, zero_division=0)
recall = recall_score(y_test_pca, y_pred, zero_division=0)
f1 = f1_score(y_test_pca, y_pred, zero_division=0)
roc_auc = roc_auc_score(y_test_pca, y_pred_proba)

metrics_dict = {
    'Accuracy': accuracy,
    'Precision': precision,
    'Recall': recall,
    'F1-Score': f1,
    'ROC-AUC': roc_auc
}

print(f"\n‚úì Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
print(f"‚úì Precision: {precision:.4f} ({precision*100:.2f}%)")
print(f"‚úì Recall:    {recall:.4f} ({recall*100:.2f}%)")
print(f"‚úì F1-Score:  {f1:.4f}")
print(f"‚úì ROC-AUC:   {roc_auc:.4f}")

# Visualize first 2 principal components
plt.figure(figsize=(10, 8))
scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_balanced,
                     cmap='coolwarm', alpha=0.6, edgecolors='black', linewidth=0.5)
plt.xlabel('First Principal Component', fontsize=12)
plt.ylabel('Second Principal Component', fontsize=12)
plt.title('PCA - First Two Components', fontsize=14, fontweight='bold')
plt.colorbar(scatter, label='Label')
plt.grid(alpha=0.3)
plt.tight_layout()
plt.show()

# === FIXED DASHBOARD (One Cell) ===
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix, roc_curve

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("Metrics Comparison", "Confusion Matrix",
                    "ROC Curve", "Prediction Probability Distribution"),
    specs=[[{"type":"bar"}, {"type":"heatmap"}],
           [{"type":"scatter"}, {"type":"histogram"}]],
    horizontal_spacing=0.15,
    vertical_spacing=0.25
)

# Metrics
fig.add_trace(go.Bar(
    x=list(metrics_dict.keys()),
    y=list(metrics_dict.values()),
    marker=dict(color=list(metrics_dict.values()), colorscale='Viridis'),
    text=[f'{v:.3f}' for v in metrics_dict.values()],
    textposition='auto',
    name='Metrics'),
    row=1, col=1
)
fig.update_xaxes(title_text="Metric", row=1, col=1)
fig.update_yaxes(title_text="Score", row=1, col=1)

# Confusion Matrix
cm = confusion_matrix(y_test_pca, y_pred)
fig.add_trace(go.Heatmap(
    z=cm, x=['Bad', 'Good'], y=['Bad', 'Good'],
    colorscale='RdYlGn', text=cm, texttemplate='%{text}',
    showscale=False,
    name='CM'),
    row=1, col=2
)
fig.update_xaxes(title_text="Predicted", row=1, col=2)
fig.update_yaxes(title_text="Actual", row=1, col=2)

# ROC Curve
fpr, tpr, _ = roc_curve(y_test_pca, y_pred_proba)
fig.add_trace(go.Scatter(
    x=fpr, y=tpr, mode='lines',
    name=f'XGBoost (AUC={roc_auc:.3f})',
    line=dict(width=3)),
    row=2, col=1
)
fig.add_trace(go.Scatter(
    x=[0, 1], y=[0, 1], mode='lines',
    name='Random', line=dict(dash='dash')),
    row=2, col=1
)
fig.update_xaxes(title_text="False Positive Rate", row=2, col=1)
fig.update_yaxes(title_text="True Positive Rate", row=2, col=1)

# Probability Distribution
fig.add_trace(go.Histogram(
    x=y_pred_proba[y_test_pca == 0], name='Bad (Actual)',
    nbinsx=30, opacity=0.6),
    row=2, col=2
)
fig.add_trace(go.Histogram(
    x=y_pred_proba[y_test_pca == 1], name='Good (Actual)',
    nbinsx=30, opacity=0.6),
    row=2, col=2
)
fig.update_xaxes(title_text="Prediction Probability", row=2, col=2)
fig.update_yaxes(title_text="Count", row=2, col=2)

# Layout
fig.update_layout(
    height=1200,
    showlegend=True,
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.12,
        xanchor="center",
        x=0.5
    ),
    margin=dict(l=60, r=60, t=80, b=60),
    font=dict(size=15)
)

fig.show()

"""Random Forest Model:"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

rf_model = RandomForestClassifier(
    n_estimators=500,
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    random_state=42,
    n_jobs=-1
)

rf_model.fit(X_train_pca, y_train_pca)

rf_pred = rf_model.predict(X_test_pca_split)
rf_pred_proba = rf_model.predict_proba(X_test_pca_split)[:, 1]

print("‚úì Random Forest training complete")

from sklearn.metrics import (
    accuracy_score, precision_score, recall_score,
    f1_score, roc_auc_score, confusion_matrix
)

rf_accuracy = accuracy_score(y_test_pca, rf_pred)
rf_precision = precision_score(y_test_pca, rf_pred, zero_division=0)
rf_recall = recall_score(y_test_pca, rf_pred, zero_division=0)
rf_f1 = f1_score(y_test_pca, rf_pred, zero_division=0)
rf_auc = roc_auc_score(y_test_pca, rf_pred_proba)

print("\nüìä Random Forest Performance")
print(f"‚úì Accuracy:  {rf_accuracy:.4f}")
print(f"‚úì Precision: {rf_precision:.4f}")
print(f"‚úì Recall:    {rf_recall:.4f}")
print(f"‚úì F1-score:  {rf_f1:.4f}")
print(f"‚úì ROC-AUC:   {rf_auc:.4f}")

print("\nConfusion Matrix:")
print(confusion_matrix(y_test_pca, rf_pred))

# === FIXED DASHBOARD (RF, One Cell) ===
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix, roc_curve

rf_metrics = {
    'Accuracy': rf_accuracy,
    'Precision': rf_precision,
    'Recall': rf_recall,
    'F1-Score': rf_f1,
    'ROC-AUC': rf_auc
}

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=("RF Metrics Comparison", "RF Confusion Matrix",
                    "RF ROC Curve", "RF Prediction Probability Distribution"),
    specs=[[{"type":"bar"}, {"type":"heatmap"}],
           [{"type":"scatter"}, {"type":"histogram"}]],
    horizontal_spacing=0.15,
    vertical_spacing=0.25
)

# 1. Metrics
fig.add_trace(go.Bar(
    x=list(rf_metrics.keys()),
    y=list(rf_metrics.values()),
    marker=dict(color=list(rf_metrics.values()), colorscale='Viridis'),
    text=[f'{v:.3f}' for v in rf_metrics.values()],
    textposition='auto',
    name='RF Metrics'),
    row=1, col=1
)
fig.update_xaxes(title_text="Metric", row=1, col=1)
fig.update_yaxes(title_text="Score", row=1, col=1)

# 2. Confusion Matrix
rf_cm = confusion_matrix(y_test_pca, rf_pred)
fig.add_trace(go.Heatmap(
    z=rf_cm, x=['Bad', 'Good'], y=['Bad', 'Good'],
    colorscale='RdYlGn', text=rf_cm, texttemplate='%{text}',
    showscale=False,
    name='RF CM'),
    row=1, col=2
)
fig.update_xaxes(title_text="Predicted", row=1, col=2)
fig.update_yaxes(title_text="Actual", row=1, col=2)

# 3. ROC Curve
rf_fpr, rf_tpr, _ = roc_curve(y_test_pca, rf_pred_proba)
fig.add_trace(go.Scatter(
    x=rf_fpr, y=rf_tpr, mode='lines',
    name=f'RF (AUC={rf_auc:.3f})',
    line=dict(width=3)),
    row=2, col=1
)
fig.add_trace(go.Scatter(
    x=[0, 1], y=[0, 1], mode='lines',
    name='Random', line=dict(dash='dash')),
    row=2, col=1
)
fig.update_xaxes(title_text="False Positive Rate", row=2, col=1)
fig.update_yaxes(title_text="True Positive Rate", row=2, col=1)

# 4. Probability Distribution
fig.add_trace(go.Histogram(
    x=rf_pred_proba[y_test_pca == 0], name='Bad (Actual)',
    nbinsx=30, opacity=0.6),
    row=2, col=2
)
fig.add_trace(go.Histogram(
    x=rf_pred_proba[y_test_pca == 1], name='Good (Actual)',
    nbinsx=30, opacity=0.6),
    row=2, col=2
)
fig.update_xaxes(title_text="Prediction Probability", row=2, col=2)
fig.update_yaxes(title_text="Count", row=2, col=2)

# Layout
fig.update_layout(
    height=1200,
    showlegend=True,
    legend=dict(
        orientation="h",
        yanchor="bottom",
        y=1.12,
        xanchor="center",
        x=0.5
    ),
    margin=dict(l=60, r=60, t=80, b=60),
    font=dict(size=15)
)

fig.show()